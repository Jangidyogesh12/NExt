{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKkR6zEvUyzE"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ShGGBR9lGfm5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import re\n",
        "from torch.nn.functional import one_hot\n",
        "import torch.optim as optim\n",
        "import os\n",
        "\n",
        "\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
        "\n",
        "#Defining the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bdBiXhJUNjmy"
      },
      "outputs": [],
      "source": [
        "# This Function tokenises the data\n",
        "def tokeniser(path):\n",
        "    # Open the file in read-write mode ('+r') with UTF-8 encoding\n",
        "    with open(path, '+r', encoding='utf-8') as file:\n",
        "        # Read the entire content of the file\n",
        "        data = file.read()\n",
        "\n",
        "        # Convert data to lowercase and replace 'old_text' with 'new_text'\n",
        "        data = data.lower()\n",
        "        new_data = data.replace('old_text', 'new_text')\n",
        "\n",
        "        # Move the file pointer to the beginning, write new_data, and truncate the rest\n",
        "        file.seek(0)\n",
        "        file.write(new_data)\n",
        "        file.truncate()\n",
        "\n",
        "        # Split the modified data into tokens\n",
        "        tokens = re.findall(r\"\\b\\w+(?:'\\w+)?\\b\", new_data)\n",
        "    # Return the list of tokens\n",
        "    return np.array(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GJu1E5rMNRYK"
      },
      "outputs": [],
      "source": [
        "path = r'/content/drive/MyDrive/Kaggle/conversion.txt'\n",
        "tokenized_data = tokeniser(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7TJSy1qS04d",
        "outputId": "0bea5ca2-161d-40aa-ef8b-faa3617c768c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8206"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Create a vocabulary from the tokenized data\n",
        "vocab = set(tokenized_data)\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DK8PEMWGICT5"
      },
      "outputs": [],
      "source": [
        "# Convert words to indices\n",
        "indexed_data = [word_to_idx[word] for word in tokenized_data]\n",
        "indexed_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NextWordDataset(Dataset):\n",
        "    def __init__(self, data, sequence_length):\n",
        "        self.data = data\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.sequence_length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        input_seq = self.data[index:index + self.sequence_length]\n",
        "        target = self.data[index + self.sequence_length]\n",
        "        return torch.tensor(input_seq), torch.tensor(target)"
      ],
      "metadata": {
        "id": "kIej54nZbCIN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM-based model\n",
        "class NextWordPredictor(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(NextWordPredictor, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        output = self.fc(lstm_out[:, -1, :])\n",
        "        return output"
      ],
      "metadata": {
        "id": "PGHM1AFGG__q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 150\n",
        "sequence_length = 3  # Length of input sequences"
      ],
      "metadata": {
        "id": "eQAtK2rW8Qhv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset and DataLoader\n",
        "dataset = NextWordDataset(indexed_data, sequence_length)\n",
        "batch_size = 1  # Using batch size 1 for simplicity\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "64gzyRvT8xMI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model and data to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = NextWordPredictor(vocab_size, embedding_dim, hidden_dim).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "k2ov83ik8zGS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for data in tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"batch\"):\n",
        "        inputs, target = data\n",
        "        inputs, target = inputs.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}] Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "print(\"Training finished.\")\n"
      ],
      "metadata": {
        "id": "A1RnBR-0GMEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd //content/drive/MyDrive/Kaggle"
      ],
      "metadata": {
        "id": "cKdldFsyL_jV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the model state dictionary\n",
        "torch.save(model, '/content/drive/MyDrive/Kaggle/model.pt')"
      ],
      "metadata": {
        "id": "4KMZdJgRYN_G"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the model\n",
        "model = NextWordPredictor(vocab_size, embedding_dim, hidden_dim)\n",
        "\n",
        "# Load the saved model state dictionary\n",
        "model.load_state_dict(torch.load('next_word_model.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsAZjOByYZPH",
        "outputId": "ec4298e8-eef1-4f72-ff02-4d9bf3c79cc8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample sequence of words\n",
        "input_sequence = [\"good\"]\n",
        "\n",
        "# Convert words to indices using the word_to_idx dictionary\n",
        "indexed_sequence = [word_to_idx[word] for word in input_sequence]\n",
        "\n",
        "# Convert the indexed sequence to a PyTorch tensor\n",
        "input_tensor = torch.tensor(indexed_sequence).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Use the model for prediction\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():  # Disable gradient calculation\n",
        "    output = model(input_tensor)\n",
        "\n",
        "# Apply softmax to get probabilities\n",
        "softmax = nn.Softmax(dim=1)\n",
        "probabilities = softmax(output)\n",
        "\n",
        "# Get the top n words based on probabilities\n",
        "top_n = 5  # Number of top words to retrieve\n",
        "top_probabilities, top_indices = torch.topk(probabilities, top_n)\n",
        "\n",
        "# Convert indices back to words using idx_to_word dictionary\n",
        "top_words = [idx_to_word[idx.item()] for idx in top_indices.squeeze()]\n",
        "\n",
        "# Display the top words and their probabilities\n",
        "for word, prob in zip(top_words, top_probabilities.squeeze()):\n",
        "    print(f\"Word: {word}\")\n"
      ],
      "metadata": {
        "id": "BD_g5x7_aemH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "06PkgpWHafgg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}